{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Zohaib Khan - 3740572"
      ],
      "metadata": {
        "id": "4D40bbbGndEm"
      },
      "id": "4D40bbbGndEm"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu langchain langchain-community sentence-transformers rank_bm25 pypdf"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qP_ZX7_IwvAg"
      },
      "id": "qP_ZX7_IwvAg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "403d61de",
      "metadata": {
        "id": "403d61de"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking"
      ],
      "metadata": {
        "id": "HwveJSw83HMQ"
      },
      "id": "HwveJSw83HMQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and split PDF\n",
        "doc_path = \"https://homel.vsb.cz/~fai0013/Kniha_Algoritmy.pdf\"\n",
        "loader = PyPDFLoader(doc_path)\n",
        "pages = loader.load()\n",
        "print(len(pages))\n",
        "\n",
        "# Chunking text\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "# Prepare documents and their metadata\n",
        "texts = [chunk.page_content for chunk in chunks]\n",
        "metadata = [chunk.metadata for chunk in chunks]\n",
        "print(len(texts))\n",
        "print(chunks[10])"
      ],
      "metadata": {
        "id": "xYXrja-U3CvR"
      },
      "id": "xYXrja-U3CvR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3c8ff7",
      "metadata": {
        "id": "3d3c8ff7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Initialize embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
        "\n",
        "# Create FAISS vector database\n",
        "vectordb = FAISS.from_documents(pages, embedding_model)\n",
        "\n",
        "# Save FAISS index to disk for later use\n",
        "vectordb.save_local(\"faiss_index\")\n",
        "\n",
        "# Check the number of stored documents\n",
        "print(f\"Number of documents in the vector store: {vectordb.index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BM25 Indexing\n",
        "tokenized_texts = [text.split() for text in texts]\n",
        "bm25 = BM25Okapi(tokenized_texts)\n",
        "\n",
        "def reciprocal_rank_fusion(results_bm25, results_embedding, k=2):\n",
        "    scores = {}\n",
        "\n",
        "    # Use document content or metadata as the key\n",
        "    for rank, (doc, score) in enumerate(results_bm25):\n",
        "        doc_id = doc.page_content  # Or use doc.metadata.get(\"source\", \"unknown\") if available\n",
        "        scores[doc_id] = scores.get(doc_id, 0) + 1 / (rank+1) # (k + rank + 1)\n",
        "        print(\"BM25\", scores[doc_id])\n",
        "\n",
        "    for rank, (doc, score) in enumerate(results_embedding):\n",
        "        doc_id = doc.page_content  # Use the same identifier\n",
        "        scores[doc_id] = scores.get(doc_id, 0) + 1 / (rank+1) # (k + rank + 1)\n",
        "        print(\"Dense\", scores[doc_id])\n",
        "\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "# Extract page content and metadata properly\n",
        "def format_response(doc):\n",
        "    return f\"Page {doc.metadata.get('page', 'Unknown')}: {doc.page_content.strip()}\""
      ],
      "metadata": {
        "id": "8IPULh4Jqe9p"
      },
      "id": "8IPULh4Jqe9p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve function\n",
        "def retrieve(query, k=3):\n",
        "    query_embedding = embedding_model.embed_query(query)\n",
        "    results_embedding = vectordb.similarity_search_with_score_by_vector(query_embedding, k=k)\n",
        "    results_embedding = sorted(results_embedding, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\"============Dense Embeddings=============\")\n",
        "    for doc, score in results_embedding:\n",
        "        print(f\"page {doc.metadata.get('page','Unknown')} - Score: {score:.4f} - {doc.page_content[:100]}...\")\n",
        "\n",
        "    # Get BM25 scores for all documents and sort to get top-k results\n",
        "    results_bm25 = [(idx, bm25.get_scores(query.split())[idx]) for idx in range(len(texts))]\n",
        "    results_bm25 = sorted(results_bm25, key=lambda x: x[1], reverse=True)[:k]  # Keep only top-k results\n",
        "    # Convert BM25 results to (Document, score) format\n",
        "    results_bm25_docs = [(Document(page_content=texts[idx], metadata=metadata[idx]), score) for idx, score in results_bm25]\n",
        "\n",
        "    print(\"************BM25 Results*************\")\n",
        "    for doc, score in results_bm25_docs:\n",
        "        print(f\"page {doc.metadata.get('page','Unknown')} - Score: {score:.4f} - {doc.page_content[:100]}...\")\n",
        "\n",
        "    # Create a lookup dictionary {document content -> Document object}\n",
        "    doc_lookup = {doc.page_content: doc for doc, _ in results_bm25_docs}\n",
        "    doc_lookup.update({doc.page_content: doc for doc, _ in results_embedding})\n",
        "\n",
        "    # Fuse results\n",
        "    fused_results = reciprocal_rank_fusion(results_bm25_docs, results_embedding)\n",
        "\n",
        "    # Format results, ensuring document IDs are mapped back to actual Documents\n",
        "    return [format_response(doc_lookup[doc_id]) for doc_id, _ in fused_results if doc_id in doc_lookup]\n",
        "\n",
        "    #fused_results = reciprocal_rank_fusion(results_bm25, results_embedding)\n",
        "    #return [(texts[idx], metadata[idx][\"page\"] if \"page\" in metadata[idx] else \"Unknown\") for idx, _ in fused_results]\n"
      ],
      "metadata": {
        "id": "qzfDu9fOqhF3"
      },
      "id": "qzfDu9fOqhF3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline(model_name, question, ground_truth):\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\", #device_map='cuda'\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "  )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "  print(\"Model Information\")\n",
        "  print(model.dtype)\n",
        "  total_params = sum(p.numel() for p in model.parameters())\n",
        "  print(f\"Total Parameters: {total_params / 1e6} million\")\n",
        "  memory_footprint = total_params * 2 / (1024 ** 2)  # Convert to MB\n",
        "  print(f\"Estimated Memory Footprint: {memory_footprint:.2f} MB\\n\")\n",
        "\n",
        "  # Create a pipeline\n",
        "  generator = pipeline(\n",
        "  \"text-generation\",\n",
        "  model=model,\n",
        "  tokenizer=tokenizer,\n",
        "  return_full_text=False,\n",
        "  max_new_tokens=5000,\n",
        "  # do_sample=False\n",
        "  )\n",
        "\n",
        "  retrieved_responses = retrieve(question, k=3)\n",
        "  semantic_response = \"\"\n",
        "  for i in range(0,len(retrieved_responses)):\n",
        "    semantic_response = semantic_response + (retrieved_responses[i])\n",
        "    semantic_response = semantic_response + \"-------\"\n",
        "\n",
        "  print(\"Now performing LLM Search\")\n",
        "\n",
        "  # Construct the RAG prompt\n",
        "  prompt = f\"\"\"\n",
        "  You are an AI assistant tasked with answering questions based on retrieved knowledge.\n",
        "\n",
        "  ### **Retrieved Information**:\n",
        "  1. {retrieved_responses[0]}\n",
        "\n",
        "  2. {retrieved_responses[1]}\n",
        "\n",
        "  3. {retrieved_responses[2]}\n",
        "\n",
        "  ### **Question**:\n",
        "  {question}\n",
        "\n",
        "  ### **Instructions**:\n",
        "  - Integrate the key points from all retrieved responses into a **cohesive, well-structured answer**.\n",
        "  - If the responses are **contradictory**, mention the different perspectives.\n",
        "  - If none of the retrieved responses contain relevant information, reply:\n",
        "  **\"I couldn't find a good response to your query in the database.\"**\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate response using LLM\n",
        "  messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "  output = generator(messages)\n",
        "  llm_response =  textwrap.fill(output[0][\"generated_text\"], width=80)\n",
        "  print(\"Responses with semantic search:\\n{}\\nResponses with LLM use:\\n{}\".format(semantic_response, llm_response))\n",
        "\n",
        "  #EVALATUATION\n",
        "  print(\"\\n\\nEVALUATION\\n\")\n",
        "    # üîç Retrieval Evaluation Prompt\n",
        "  eval_prompt = f\"\"\"\n",
        "  You are an expert evaluator.\n",
        "\n",
        "  ### Task:\n",
        "  Assess the quality of the retrieved information used to answer the question.\n",
        "\n",
        "  ### Question:\n",
        "  {question}\n",
        "\n",
        "  ### Retrieved Context:\n",
        "  1. {retrieved_responses[0]}\n",
        "  2. {retrieved_responses[1]}\n",
        "  3. {retrieved_responses[2]}\n",
        "\n",
        "  ### AI-Generated Answer:\n",
        "  {llm_response}\n",
        "\n",
        "  ---\n",
        "\n",
        "  PART 1: Relevance of Retrieval\n",
        "  - Assess whether each chunk is relevant to the question.\n",
        "  - For each chunk, state:\n",
        "  - Relevance (Yes/No)\n",
        "  - Reason\n",
        "\n",
        "  PART 2: Faithfulness of Retrieval\n",
        "  - Break the generated answer into **distinct factual claims**.\n",
        "  - For each claim:\n",
        "  - Claim text\n",
        "  - Is it supported by retrieved content? (Yes/No)\n",
        "  - Which chunk(s) support it (if any)\n",
        "\n",
        "  - Then calculate:\n",
        "  Faithfulness Score = (Number of Supported Claims) / (Total Claims)\n",
        "\n",
        "  PART 3: LLM Response\n",
        "  Please rate the generated answer on a scale of 1 to 5 for each of the following:\n",
        "\n",
        "  - Correctness: Is it factually accurate compared to the ground truth?\n",
        "  - Relevance: Does it focus on the core points?\n",
        "  - Coherence: Is it logically and clearly written?\n",
        "  - Completeness: Does it match the full scope of the ground truth?\n",
        "  - Faithfulness: Does the answer stay grounded in the retrieved context and avoid hallucinations?\n",
        "\n",
        "  ### Format:\n",
        "  PART 1: Relevance of Chunks\n",
        "  - Chunk 1: Relevant: <Yes/No> ‚Äì <reason>\n",
        "  - Chunk 2: Relevant: <Yes/No> ‚Äì <reason>\n",
        "  - Chunk 3: Relevant: <Yes/No> ‚Äì <reason>\n",
        "\n",
        "  PART 2: Faithfulness\n",
        "  - Claim 1: \"...\", Supported: Yes, Source: Chunk 2\n",
        "  - Claim 2: \"...\", Supported: No\n",
        "  ...\n",
        "  Faithfulness Score: X/Y = Z.ZZ\n",
        "\n",
        "  PART 3: LLM Response\n",
        "  -Correctness: <score>/5 - <comment>\n",
        "  -Relevance: <score>/5 - <comment>\n",
        "  -Coherence: <score>/5 - <comment>\n",
        "  -Completeness: <score>/5 - <comment>\n",
        "  -Faithfulness: <score>/5 - <comment>\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  eval = generator([{\"role\": \"user\", \"content\": eval_prompt}])\n",
        "  print(\"\\n Retrieval Evaluation:\")\n",
        "  print(\"=\"*35)\n",
        "  print(eval[0][\"generated_text\"])\n",
        "  print(\"=\"*35)\n",
        "\n",
        "  return semantic_response, llm_response"
      ],
      "metadata": {
        "id": "aYUmOiMVDR0U"
      },
      "id": "aYUmOiMVDR0U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = [\n",
        "  \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "  \"microsoft/phi-2\",\n",
        "  \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "]\n",
        "\n",
        "questions = [\n",
        "    \"What is the Master Theorem?\",\n",
        "    \"Explain what is meant by Divide and Conquer algorithms.\",\n",
        "    \"What is the Knapsack Problem?\"\n",
        "]\n",
        "\n",
        "ground_truths = [\n",
        "       \"Master Theorem: If f(n) ‚àà Œò(n^d) where d ‚â• 0 in recurrence (5.1), then\\n\"\n",
        "    \"T(n) ‚àà\\n\"\n",
        "    \"    Œò(n^d)         if a < b^d,\\n\"\n",
        "    \"    Œò(n^d log n)   if a = b^d,\\n\"\n",
        "    \"    Œò(n^log_b a)   if a > b^d.\\n\"\n",
        "    \"Analogous results hold for the O and Œ© notations, too.\",\n",
        "\n",
        "    \"Divide-and-conquer is a general algorithm design technique that solves a problem by dividing it into several smaller subproblems of the same type (ideally, of about equal size), solving each of them recursively, and then combining their solutions to get a solution to the original problem. Many efficient algorithms are based on this technique, although it can be both inapplicable and inferior to simpler algorithmic solutions.\",\n",
        "\n",
        "    \"The knapsack problem can be posed as follows. Given a knapsack of capacity W and n items of weights w1,...,wn and values v1,...,vn, find the most valuable subset of the items that fits into the knapsack.\",\n",
        "]"
      ],
      "metadata": {
        "id": "pPBtXgp-FBoz"
      },
      "id": "pPBtXgp-FBoz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(model_names[0], questions[0], ground_truths[0] )"
      ],
      "metadata": {
        "id": "PphJ-kAEgJ8s"
      },
      "id": "PphJ-kAEgJ8s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(model_names[0], questions[1], ground_truths[1] )"
      ],
      "metadata": {
        "id": "orV5JHwdgKn1"
      },
      "id": "orV5JHwdgKn1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(model_names[0], questions[2], ground_truths[2] )"
      ],
      "metadata": {
        "id": "FC78uoqIgLH8"
      },
      "id": "FC78uoqIgLH8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(model_names[1], questions[0], ground_truths[0] )"
      ],
      "metadata": {
        "id": "5pHeAFyUgOMK"
      },
      "id": "5pHeAFyUgOMK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(model_names[1], questions[1], ground_truths[1] )"
      ],
      "metadata": {
        "id": "DFdB-p2CgQJq"
      },
      "id": "DFdB-p2CgQJq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(model_names[1], questions[2], ground_truths[2] )"
      ],
      "metadata": {
        "id": "rFlKjKPkgQn3"
      },
      "id": "rFlKjKPkgQn3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(model_names[2], questions[0], ground_truths[0] )"
      ],
      "metadata": {
        "id": "RxfCEj2lgU0m"
      },
      "id": "RxfCEj2lgU0m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(model_names[2], questions[1], ground_truths[1] )"
      ],
      "metadata": {
        "id": "nzmsQwuGgVjq"
      },
      "id": "nzmsQwuGgVjq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(model_names[2], questions[2], ground_truths[2] )"
      ],
      "metadata": {
        "id": "t2EuKKhGgV8R"
      },
      "id": "t2EuKKhGgV8R",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}